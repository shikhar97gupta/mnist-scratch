class Forward_Feed:
    def relu(self,X):
        X[X<0]=0
    def softmax(self,X):
        _X = np.zeros_like(X)
        for index in range(X.shape[0]):
            _X[index] = np.e**(X[index])/np.sum(np.e**(X[index]))
        return _X
    def cross_entropy(self,layers,labels):
        loss = -np.sum(np.multiply(labels,np.log(layers[-1])),axis=1)
        return loss
    def feed_forward(self,layers,weights,biases):
        for index in range(len(weights)):
            layers[index+1] = np.dot(layers[index],weights[index])+biases[index]
            if(index!=len(weights)-1):
                self.relu(layers[index+1])
            else:
                layers[-1] = self.softmax(layers[index+1])
        #loss = np.mean(self.cross_entropy(layers,labels))
        #return loss
